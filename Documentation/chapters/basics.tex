\section{Basics and Theory}
\label{sec:basics-theory}

This chapter introduces the core concepts required to understand the thesis, including schema evolution, schema matching and mapping, ETL pipeline operation, and retrieval-augmented generation (RAG) with large language models (LLMs). The detailed literature survey is presented in Chapter~3 (Related Work), while this chapter provides foundational definitions and terminology used throughout the thesis.

\subsection{Schema Evolution}

\textbf{Schema evolution} refers to changes in a dataset's structure over time. In practice, evolution can include adding or removing attributes, renaming fields, changing data types, and restructuring nested records. Such changes are common in data integration environments and directly impact mappings and transformation logic \cite{velegrakis2003mapping,yu2005semantic}.

\subsection{Schema Matching and Mapping}

\textbf{Schema matching} aims to identify correspondences between two schemas, often using name similarity, structural context, and instance-level evidence \cite{rahm2001survey}. \textbf{Schema mappings} then specify how data in a source schema corresponds to data in a target schema, enabling transformation and integration. When schemas evolve, these mappings must be adapted or regenerated.

\subsection{ETL Pipelines and Orchestration}

ETL pipelines operationalize data movement and transformation into analytical systems. In production, these pipelines are typically orchestrated as workflows with dependencies, retries, monitoring, and scheduling. While orchestration improves reliability of execution, it does not remove the underlying challenge of maintaining transformation correctness under schema evolution.

\subsection{Large Language Models and Retrieval-Augmented Generation}

Recent LLMs have demonstrated strong ability to generate and refactor code \cite{chen2021evaluating}. However, pure generation can hallucinate or omit important constraints. Retrieval-augmented generation (RAG) addresses this by retrieving relevant external context (e.g., documentation, examples, historical mappings) and conditioning generation on that evidence \cite{lewis2020retrieval}. This thesis applies RAG to ground mapping regeneration in previously validated artifacts and schema-evolution examples.

\section{Dataset Used in This Thesis}
\label{sec:thesis-dataset}

The concepts introduced in the Basics and Theory chapter are instantiated in this thesis using a taxi trip dataset inspired by the New York City Taxi and Limousine Commission (TLC) data. Instead of directly using full production-scale files, we construct three controlled versions of the dataset in Python to capture typical schema evolution patterns. Each version contains attributes such as pickup and dropoff times, trip distance, passenger counts, vendor identifiers, and payment-related fields, but with deliberate differences across versions.

In the first version (\texttt{v1}), the schema reflects an early design with identifiers for vehicles and drivers, timestamps, coordinates, and basic trip metrics like \texttt{trip\_time\_in\_secs} and \texttt{trip\_distance}. The second version (\texttt{v2}) introduces column renames (for example, \texttt{trip\_time\_in\_secs} is replaced by \texttt{trip\_duration}) and adds new payment and fare-related attributes. The third version (\texttt{v3}) further evolves the schema by replacing raw longitude/latitude coordinates with location identifiers and by adding additional surcharge fields. These variations are generated and analyzed in the Jupyter notebook \texttt{nyc\_taxi\_dataset\_analysis.ipynb}.

For each version, we extract a structured schema description from the corresponding pandas DataFrame, including column names, data types, nullability, uniqueness statistics, and example values. We then compare schema pairs (\texttt{v1}~$\rightarrow$~\texttt{v2}, \texttt{v2}~$\rightarrow$~\texttt{v3}) to identify added and removed columns, potential renames, and type changes. The resulting schemas and change sets are saved as JSON files in the \texttt{schemas/} directory and serve as concrete inputs for the schema evolution detection and mapping regeneration components implemented elsewhere in the project.

In addition to schema-level analysis, we compute basic data characteristics such as dataset shapes, memory usage, missing values, duplicate rows, and summary statistics for numeric attributes. These exploratory steps mirror typical ``tools and methods'' practice: they validate that the synthetic data behaves realistically and help to reason about the impact of schema changes on ETL logic. The same extracted schemas and detected changes are later reused in the Airflow-based ETL pipeline and in experiments where RAG-enhanced LLMs generate and adapt transformation code based on historical mappings and schema versions.