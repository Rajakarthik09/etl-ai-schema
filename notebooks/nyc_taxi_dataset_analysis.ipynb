{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NYC Taxi Dataset Analysis for Thesis Research\n",
        "\n",
        "## Adaptive Schema Evolution Detection and Mapping Regeneration in ETL Pipelines\n",
        "\n",
        "This notebook demonstrates how the NYC Taxi Trip dataset can be used for evaluating adaptive schema evolution detection and mapping regeneration in ETL pipelines using Retrieval-Augmented Large Language Models.\n",
        "\n",
        "### Dataset Overview\n",
        "The NYC Taxi Trip dataset is maintained by the New York City Taxi and Limousine Commission (TLC) and provides an ideal testbed for schema evolution research due to:\n",
        "- Multiple versions with documented schema changes\n",
        "- Real-world data quality challenges\n",
        "- Sufficient volume for scalability testing\n",
        "- Public availability for reproducibility\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset Acquisition and Schema Versions\n",
        "\n",
        "The NYC Taxi dataset has evolved over time with significant schema changes. We'll simulate different versions to demonstrate schema evolution patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample data representing different schema versions\n",
        "# Version 1: Early schema (2010-2014)\n",
        "def create_taxi_v1(n_records=1000):\n",
        "    \"\"\"Create Version 1 of NYC Taxi dataset with early schema\"\"\"\n",
        "    np.random.seed(42)\n",
        "    data = {\n",
        "        'medallion': [f'med_{i:06d}' for i in range(n_records)],\n",
        "        'hack_license': [f'hack_{np.random.randint(100000, 999999)}' for _ in range(n_records)],\n",
        "        'vendor_id': np.random.choice(['VTS', 'CMT', 'DDS'], n_records),\n",
        "        'rate_code': np.random.choice([1, 2, 3, 4, 5, 6], n_records),\n",
        "        'store_and_fwd_flag': np.random.choice(['Y', 'N'], n_records),\n",
        "        'pickup_datetime': pd.date_range('2014-01-01', periods=n_records, freq='5min'),\n",
        "        'dropoff_datetime': pd.date_range('2014-01-01 00:10', periods=n_records, freq='5min'),\n",
        "        'passenger_count': np.random.randint(1, 6, n_records),\n",
        "        'trip_time_in_secs': np.random.randint(60, 3600, n_records),\n",
        "        'trip_distance': np.random.uniform(0.1, 20.0, n_records).round(2),\n",
        "        'pickup_longitude': np.random.uniform(-74.05, -73.85, n_records),\n",
        "        'pickup_latitude': np.random.uniform(40.6, 40.9, n_records),\n",
        "        'dropoff_longitude': np.random.uniform(-74.05, -73.85, n_records),\n",
        "        'dropoff_latitude': np.random.uniform(40.6, 40.9, n_records),\n",
        "    }\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Version 2: Updated schema (2015-2016) with schema changes\n",
        "def create_taxi_v2(n_records=1000):\n",
        "    \"\"\"Create Version 2 with schema changes: renamed columns, new columns\"\"\"\n",
        "    np.random.seed(42)\n",
        "    data = {\n",
        "        'medallion': [f'med_{i:06d}' for i in range(n_records)],\n",
        "        'hack_license': [f'hack_{np.random.randint(100000, 999999)}' for _ in range(n_records)],\n",
        "        'vendor_id': np.random.choice(['VTS', 'CMT', 'DDS'], n_records),\n",
        "        'rate_code': np.random.choice([1, 2, 3, 4, 5, 6], n_records),\n",
        "        'store_and_fwd_flag': np.random.choice(['Y', 'N'], n_records),\n",
        "        'pickup_datetime': pd.date_range('2015-01-01', periods=n_records, freq='5min'),\n",
        "        'dropoff_datetime': pd.date_range('2015-01-01 00:10', periods=n_records, freq='5min'),\n",
        "        'passenger_count': np.random.randint(1, 6, n_records),\n",
        "        'trip_duration': np.random.randint(60, 3600, n_records),  # RENAMED from trip_time_in_secs\n",
        "        'trip_distance': np.random.uniform(0.1, 20.0, n_records).round(2),\n",
        "        'pickup_longitude': np.random.uniform(-74.05, -73.85, n_records),\n",
        "        'pickup_latitude': np.random.uniform(40.6, 40.9, n_records),\n",
        "        'dropoff_longitude': np.random.uniform(-74.05, -73.85, n_records),\n",
        "        'dropoff_latitude': np.random.uniform(40.6, 40.9, n_records),\n",
        "        # NEW COLUMNS ADDED\n",
        "        'payment_type': np.random.choice([1, 2, 3, 4, 5, 6], n_records),\n",
        "        'fare_amount': np.random.uniform(2.5, 50.0, n_records).round(2),\n",
        "        'surcharge': np.random.uniform(0.0, 1.0, n_records).round(2),\n",
        "        'mta_tax': np.random.choice([0.0, 0.5], n_records),\n",
        "        'tip_amount': np.random.uniform(0.0, 10.0, n_records).round(2),\n",
        "        'tolls_amount': np.random.uniform(0.0, 5.0, n_records).round(2),\n",
        "        'total_amount': np.random.uniform(3.0, 60.0, n_records).round(2),\n",
        "    }\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Version 3: Latest schema (2017+) with further changes\n",
        "def create_taxi_v3(n_records=1000):\n",
        "    \"\"\"Create Version 3 with additional schema changes\"\"\"\n",
        "    np.random.seed(42)\n",
        "    data = {\n",
        "        'medallion': [f'med_{i:06d}' for i in range(n_records)],\n",
        "        'hack_license': [f'hack_{np.random.randint(100000, 999999)}' for _ in range(n_records)],\n",
        "        'vendor_id': np.random.choice(['VTS', 'CMT', 'DDS'], n_records),\n",
        "        'rate_code': np.random.choice([1, 2, 3, 4, 5, 6], n_records),\n",
        "        'store_and_fwd_flag': np.random.choice(['Y', 'N'], n_records),\n",
        "        'pickup_datetime': pd.date_range('2017-01-01', periods=n_records, freq='5min'),\n",
        "        'dropoff_datetime': pd.date_range('2017-01-01 00:10', periods=n_records, freq='5min'),\n",
        "        'passenger_count': np.random.randint(1, 6, n_records),\n",
        "        'trip_duration': np.random.randint(60, 3600, n_records),\n",
        "        'trip_distance': np.random.uniform(0.1, 20.0, n_records).round(2),\n",
        "        # CHANGED: Coordinates replaced with location IDs\n",
        "        'pickup_location_id': np.random.randint(1, 265, n_records),\n",
        "        'dropoff_location_id': np.random.randint(1, 265, n_records),\n",
        "        'payment_type': np.random.choice([1, 2, 3, 4, 5, 6], n_records),\n",
        "        'fare_amount': np.random.uniform(2.5, 50.0, n_records).round(2),\n",
        "        'surcharge': np.random.uniform(0.0, 1.0, n_records).round(2),\n",
        "        'mta_tax': np.random.choice([0.0, 0.5], n_records),\n",
        "        'tip_amount': np.random.uniform(0.0, 10.0, n_records).round(2),\n",
        "        'tolls_amount': np.random.uniform(0.0, 5.0, n_records).round(2),\n",
        "        'total_amount': np.random.uniform(3.0, 60.0, n_records).round(2),\n",
        "        # NEW COLUMNS ADDED\n",
        "        'improvement_surcharge': np.random.uniform(0.0, 0.3, n_records).round(2),\n",
        "        'congestion_surcharge': np.random.uniform(0.0, 2.5, n_records).round(2),\n",
        "    }\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Create datasets\n",
        "print(\"Creating dataset versions...\")\n",
        "taxi_v1 = create_taxi_v1(1000)\n",
        "taxi_v2 = create_taxi_v2(1000)\n",
        "taxi_v3 = create_taxi_v3(1000)\n",
        "\n",
        "print(f\"✓ Version 1 created: {len(taxi_v1)} records, {len(taxi_v1.columns)} columns\")\n",
        "print(f\"✓ Version 2 created: {len(taxi_v2)} records, {len(taxi_v2.columns)} columns\")\n",
        "print(f\"✓ Version 3 created: {len(taxi_v3)} records, {len(taxi_v3.columns)} columns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Schema Comparison and Evolution Detection\n",
        "\n",
        "This section demonstrates how schema evolution can be detected and analyzed, which is central to the thesis research.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Schema extraction function\n",
        "def extract_schema(df, version_name):\n",
        "    \"\"\"Extract schema information from a DataFrame\"\"\"\n",
        "    schema = {\n",
        "        'version': version_name,\n",
        "        'columns': {},\n",
        "        'column_count': len(df.columns),\n",
        "        'row_count': len(df),\n",
        "        'dtypes': {}\n",
        "    }\n",
        "    \n",
        "    for col in df.columns:\n",
        "        schema['columns'][col] = {\n",
        "            'dtype': str(df[col].dtype),\n",
        "            'nullable': df[col].isna().any(),\n",
        "            'unique_count': df[col].nunique(),\n",
        "            'sample_values': df[col].dropna().head(3).tolist() if not df[col].dropna().empty else []\n",
        "        }\n",
        "        schema['dtypes'][col] = str(df[col].dtype)\n",
        "    \n",
        "    return schema\n",
        "\n",
        "# Extract schemas\n",
        "schema_v1 = extract_schema(taxi_v1, 'v1')\n",
        "schema_v2 = extract_schema(taxi_v2, 'v2')\n",
        "schema_v3 = extract_schema(taxi_v3, 'v3')\n",
        "\n",
        "print(\"Schema Extraction Results:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nVersion 1 Schema:\")\n",
        "print(f\"  Columns: {schema_v1['column_count']}\")\n",
        "print(f\"  Column names: {list(schema_v1['columns'].keys())}\")\n",
        "\n",
        "print(f\"\\nVersion 2 Schema:\")\n",
        "print(f\"  Columns: {schema_v2['column_count']}\")\n",
        "print(f\"  Column names: {list(schema_v2['columns'].keys())}\")\n",
        "\n",
        "print(f\"\\nVersion 3 Schema:\")\n",
        "print(f\"  Columns: {schema_v3['column_count']}\")\n",
        "print(f\"  Column names: {list(schema_v3['columns'].keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Schema comparison function\n",
        "def compare_schemas(old_schema, new_schema):\n",
        "    \"\"\"Compare two schemas and detect changes\"\"\"\n",
        "    old_cols = set(old_schema['columns'].keys())\n",
        "    new_cols = set(new_schema['columns'].keys())\n",
        "    \n",
        "    changes = {\n",
        "        'added_columns': list(new_cols - old_cols),\n",
        "        'removed_columns': list(old_cols - new_cols),\n",
        "        'common_columns': list(old_cols & new_cols),\n",
        "        'type_changes': [],\n",
        "        'renamed_columns': []\n",
        "    }\n",
        "    \n",
        "    # Check for type changes in common columns\n",
        "    for col in changes['common_columns']:\n",
        "        old_type = old_schema['columns'][col]['dtype']\n",
        "        new_type = new_schema['columns'][col]['dtype']\n",
        "        if old_type != new_type:\n",
        "            changes['type_changes'].append({\n",
        "                'column': col,\n",
        "                'old_type': old_type,\n",
        "                'new_type': new_type\n",
        "            })\n",
        "    \n",
        "    # Detect potential renames (simple heuristic: similar names)\n",
        "    from difflib import SequenceMatcher\n",
        "    removed = old_cols - new_cols\n",
        "    added = new_cols - old_cols\n",
        "    \n",
        "    for old_col in removed:\n",
        "        for new_col in added:\n",
        "            similarity = SequenceMatcher(None, old_col.lower(), new_col.lower()).ratio()\n",
        "            if similarity > 0.6:  # Threshold for potential rename\n",
        "                changes['renamed_columns'].append({\n",
        "                    'old_name': old_col,\n",
        "                    'new_name': new_col,\n",
        "                    'similarity': similarity\n",
        "                })\n",
        "    \n",
        "    return changes\n",
        "\n",
        "# Compare schemas\n",
        "print(\"Schema Comparison: V1 → V2\")\n",
        "print(\"=\" * 60)\n",
        "changes_v1_v2 = compare_schemas(schema_v1, schema_v2)\n",
        "print(f\"Added columns: {changes_v1_v2['added_columns']}\")\n",
        "print(f\"Removed columns: {changes_v1_v2['removed_columns']}\")\n",
        "print(f\"Renamed columns: {changes_v1_v2['renamed_columns']}\")\n",
        "print(f\"Type changes: {changes_v1_v2['type_changes']}\")\n",
        "\n",
        "print(\"\\nSchema Comparison: V2 → V3\")\n",
        "print(\"=\" * 60)\n",
        "changes_v2_v3 = compare_schemas(schema_v2, schema_v3)\n",
        "print(f\"Added columns: {changes_v2_v3['added_columns']}\")\n",
        "print(f\"Removed columns: {changes_v2_v3['removed_columns']}\")\n",
        "print(f\"Renamed columns: {changes_v2_v3['renamed_columns']}\")\n",
        "print(f\"Type changes: {changes_v2_v3['type_changes']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Analysis and Characteristics\n",
        "\n",
        "Understanding the data characteristics is crucial for ETL pipeline design and schema evolution handling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "print(\"Dataset Statistics\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for name, df in [(\"Version 1\", taxi_v1), (\"Version 2\", taxi_v2), (\"Version 3\", taxi_v3)]:\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Shape: {df.shape}\")\n",
        "    print(f\"  Memory usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
        "    print(f\"  Missing values: {df.isnull().sum().sum()}\")\n",
        "    print(f\"  Duplicate rows: {df.duplicated().sum()}\")\n",
        "\n",
        "# Statistical summary for numeric columns\n",
        "print(\"\\n\\nNumeric Column Statistics (Version 2):\")\n",
        "print(\"=\" * 60)\n",
        "numeric_cols = taxi_v2.select_dtypes(include=[np.number]).columns\n",
        "print(taxi_v2[numeric_cols].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Relevance to Thesis Research\n",
        "\n",
        "### How This Dataset Supports the Research\n",
        "\n",
        "1. **Schema Evolution Patterns**: The dataset demonstrates real-world schema evolution including:\n",
        "   - Column additions (payment fields, surcharges)\n",
        "   - Column renames (trip_time_in_secs → trip_duration)\n",
        "   - Structural changes (coordinates → location IDs)\n",
        "   - Type changes and precision modifications\n",
        "\n",
        "2. **ETL Pipeline Scenarios**: The dataset represents typical source data requiring:\n",
        "   - Extraction from multiple versions\n",
        "   - Transformation to handle schema changes\n",
        "   - Loading into data warehouses with consistent schemas\n",
        "\n",
        "3. **RAG-Enhanced LLM Application**: \n",
        "   - Historical schema mappings can be retrieved as examples\n",
        "   - LLMs can learn patterns from previous schema evolutions\n",
        "   - Context-aware mapping generation using retrieved examples\n",
        "\n",
        "4. **Evaluation Metrics**:\n",
        "   - Detection accuracy for different change types\n",
        "   - Mapping correctness for various schema transformations\n",
        "   - Performance impact of adaptive updates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save schemas for use in thesis research\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Create schemas directory if it doesn't exist\n",
        "schemas_dir = '../schemas'\n",
        "os.makedirs(schemas_dir, exist_ok=True)\n",
        "\n",
        "# Save schemas as JSON\n",
        "with open(f'{schemas_dir}/taxi_schema_v1.json', 'w') as f:\n",
        "    json.dump(schema_v1, f, indent=2, default=str)\n",
        "\n",
        "with open(f'{schemas_dir}/taxi_schema_v2.json', 'w') as f:\n",
        "    json.dump(schema_v2, f, indent=2, default=str)\n",
        "\n",
        "with open(f'{schemas_dir}/taxi_schema_v3.json', 'w') as f:\n",
        "    json.dump(schema_v3, f, indent=2, default=str)\n",
        "\n",
        "# Save change detection results\n",
        "with open(f'{schemas_dir}/taxi_changes_v1_v2.json', 'w') as f:\n",
        "    json.dump(changes_v1_v2, f, indent=2, default=str)\n",
        "\n",
        "with open(f'{schemas_dir}/taxi_changes_v2_v3.json', 'w') as f:\n",
        "    json.dump(changes_v2_v3, f, indent=2, default=str)\n",
        "\n",
        "print(\"✓ Schemas and change detection results saved to schemas/ directory\")\n",
        "print(\"\\nThese files can be used for:\")\n",
        "print(\"  - Schema evolution detection evaluation\")\n",
        "print(\"  - RAG retrieval examples\")\n",
        "print(\"  - LLM mapping generation testing\")\n",
        "print(\"  - ETL pipeline validation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Integration with ETL Pipeline\n",
        "\n",
        "The detected schema changes can be used to automatically regenerate ETL mappings using RAG-enhanced LLMs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: How schema changes would trigger ETL mapping regeneration\n",
        "print(\"ETL Mapping Regeneration Scenario\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nWhen schema changes are detected:\")\n",
        "print(\"1. Schema detection identifies changes (as shown above)\")\n",
        "print(\"2. RAG system retrieves similar schema evolution examples\")\n",
        "print(\"3. LLM generates new ETL transform mappings\")\n",
        "print(\"4. Generated mappings are validated\")\n",
        "print(\"5. ETL pipeline is updated automatically\")\n",
        "\n",
        "print(\"\\n\\nExample Mapping Requirements (V1 → V2):\")\n",
        "print(\"- Map 'trip_time_in_secs' → 'trip_duration'\")\n",
        "print(\"- Add default values for new payment columns\")\n",
        "print(\"- Handle new fare-related columns\")\n",
        "print(\"- Maintain existing business logic\")\n",
        "\n",
        "print(\"\\n\\nExample Mapping Requirements (V2 → V3):\")\n",
        "print(\"- Map coordinate columns to location IDs\")\n",
        "print(\"- Add new surcharge fields\")\n",
        "print(\"- Update data type handling\")\n",
        "print(\"- Preserve existing transformations\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary and Next Steps\n",
        "\n",
        "This notebook has demonstrated:\n",
        "\n",
        "1. ✅ Dataset creation with schema evolution patterns\n",
        "2. ✅ Schema extraction and comparison\n",
        "3. ✅ Change detection (additions, removals, renames, type changes)\n",
        "4. ✅ Data analysis and characteristics\n",
        "5. ✅ Relevance to thesis research objectives\n",
        "\n",
        "### Next Steps for Thesis Research:\n",
        "\n",
        "1. **Implement RAG System**: Build retrieval system for schema evolution examples\n",
        "2. **LLM Integration**: Connect with LLM APIs for mapping generation\n",
        "3. **Evaluation Framework**: Develop metrics for detection accuracy and mapping correctness\n",
        "4. **ETL Pipeline Integration**: Integrate with Apache Airflow for automated updates\n",
        "5. **Experiments**: Run comprehensive experiments on multiple schema evolution scenarios\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
